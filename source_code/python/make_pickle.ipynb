{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "toc": true
   },
   "source": [
    "<h1>Table of Contents<span class=\"tocSkip\"></span></h1>\n",
    "<div class=\"toc\"><ul class=\"toc-item\"><li><span><a href=\"#Get-Data\" data-toc-modified-id=\"Get-Data-1\">Get Data</a></span></li><li><span><a href=\"#Data-Preprocessing\" data-toc-modified-id=\"Data-Preprocessing-2\">Data Preprocessing</a></span></li><li><span><a href=\"#Model-Performance\" data-toc-modified-id=\"Model-Performance-3\">Model Performance</a></span></li><li><span><a href=\"#Grid-Search\" data-toc-modified-id=\"Grid-Search-4\">Grid Search</a></span><ul class=\"toc-item\"><li><span><a href=\"#Logistic-Regression\" data-toc-modified-id=\"Logistic-Regression-4.1\">Logistic Regression</a></span></li><li><span><a href=\"#Support-Vector-Classifier\" data-toc-modified-id=\"Support-Vector-Classifier-4.2\">Support Vector Classifier</a></span></li></ul></li><li><span><a href=\"#Validation-Curve\" data-toc-modified-id=\"Validation-Curve-5\">Validation Curve</a></span><ul class=\"toc-item\"><li><span><a href=\"#Logistic-Regression\" data-toc-modified-id=\"Logistic-Regression-5.1\">Logistic Regression</a></span></li><li><span><a href=\"#Support-Vector-Classifier\" data-toc-modified-id=\"Support-Vector-Classifier-5.2\">Support Vector Classifier</a></span></li></ul></li></ul></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note**: This notebook is **not a report**. This notebook is solely to construct pickles, therefore it contains no description to increase performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 52
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 5607,
     "status": "ok",
     "timestamp": 1599298622547,
     "user": {
      "displayName": "PolarBear Yap",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Gh6xY6wDRASo9ifgNeDcwMeBd3TCQFVGw14HLGtCg=s64",
      "userId": "07391035732959922581"
     },
     "user_tz": -480
    },
    "id": "xjdzFReXcIO-",
    "lines_to_next_cell": 2,
    "outputId": "f0834459-1dd6-45a4-94f9-216bd3ac6b33"
   },
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pickle\n",
    "import re\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn.compose import make_column_transformer\n",
    "from sklearn.discriminant_analysis import QuadraticDiscriminantAnalysis\n",
    "from sklearn.ensemble import (\n",
    "    AdaBoostClassifier,\n",
    "    ExtraTreesClassifier,\n",
    "    RandomForestClassifier,\n",
    ")\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import (\n",
    "    GridSearchCV,\n",
    "    RandomizedSearchCV,\n",
    "    StratifiedShuffleSplit,\n",
    "    cross_validate,\n",
    "    validation_curve,\n",
    ")\n",
    "from sklearn.naive_bayes import ComplementNB\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.preprocessing import (\n",
    "    FunctionTransformer,\n",
    "    MinMaxScaler,\n",
    "    RobustScaler,\n",
    ")\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.tree import DecisionTreeClassifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "V8yVBoUxNq1H"
   },
   "source": [
    "# Get Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "hAax9YHibqLa"
   },
   "outputs": [],
   "source": [
    "BANK_DATA_FILE_PATH = '../../datasets/speed_dating.csv'\n",
    "BANK_DATA_URL = 'https://raw.githubusercontent.com/polarBearYap/AI_Assignment/master/Datasets/speeddating.csv'\n",
    "\n",
    "\n",
    "def fetch_data_from_website(path):\n",
    "    return pd.read_csv(path, low_memory=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "R_kFejV3b9h8"
   },
   "outputs": [],
   "source": [
    "dating = fetch_data_from_website(BANK_DATA_FILE_PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "CNSOz-Y1a0tk"
   },
   "source": [
    "# Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "B4PGJD-jj9cp"
   },
   "outputs": [],
   "source": [
    "preproccessed_features = [feature for feature in dating.columns if feature.lower()[\n",
    "    :2] == 'd_']\n",
    "\n",
    "irrelevant_features = ['has_null', 'wave', 'expected_happy_with_sd_people', 'expected_num_interested_in_me',\n",
    "                       'expected_num_matches', 'field', 'decision']\n",
    "\n",
    "self_interest_feature = ['sports', 'tvsports', 'exercise', 'dining', 'museums', 'art', 'hiking', 'gaming',\n",
    "                         'clubbing', 'reading', 'tv', 'theater', 'movies', 'concerts', 'music', 'shopping', 'yoga']\n",
    "\n",
    "partner_features = ['age_o', 'race_o', 'pref_o_attractive', 'pref_o_sincere', 'pref_o_intelligence',\n",
    "                    'pref_o_funny', 'pref_o_ambitious', 'pref_o_shared_interests', 'attractive_o', 'sinsere_o', 'intelligence_o', 'funny_o', 'ambitous_o', 'shared_interests_o']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "BO7do5m9lVu7"
   },
   "outputs": [],
   "source": [
    "new_dating = dating.drop(columns=preproccessed_features +\n",
    "                         irrelevant_features+self_interest_feature+partner_features, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "FZieDiQQQqjS"
   },
   "outputs": [],
   "source": [
    "class DataPreprocessTransformer(BaseEstimator, TransformerMixin):\n",
    "\n",
    "    def __init__(self, y_feature):\n",
    "        self.y_feature = y_feature\n",
    "        self.features_with_wrong_data_type = []\n",
    "        self.numerical_features = []\n",
    "        self.categorical_features = []\n",
    "        self.features_with_invalid_value = []\n",
    "        self.one_hot_features = []\n",
    "\n",
    "    def getNumericalFeatures(self):\n",
    "        return self.numerical_features\n",
    "\n",
    "    def getCategoricalFeatures(self):\n",
    "        return self.categorical_features\n",
    "\n",
    "    def detect_int_value(self, data):\n",
    "        return np.any(data.astype(str).str.contains('^\\d+$', regex=True))\n",
    "\n",
    "    def detect_float_value(self, data):\n",
    "        return np.any(data.astype(str).str.contains('^-?\\d+\\.\\d+$|^\\d+$', regex=True))\n",
    "\n",
    "    def get_invalid_int_value(self, data):\n",
    "        return ', '.join(data[~data.astype(str).str.contains('^\\d+$', regex=True)].value_counts().index.to_list())\n",
    "\n",
    "    def get_invalid_float_value(self, data):\n",
    "        return ', '.join(data[~data.astype(str).str.contains('^-?\\d+\\.\\d+$|^\\d+$', regex=True)].value_counts().index.to_list())\n",
    "\n",
    "    def drop_rows_with_unknow_values(self, data, feature):\n",
    "        return data[~data[feature].isna()]\n",
    "\n",
    "    def find_invalid_values(self, data):\n",
    "        invalid_values = set()\n",
    "        for feature in data.columns.values:\n",
    "            if data[feature].dtype == 'object':\n",
    "                if self.detect_float_value(data[feature]):\n",
    "                    data[feature] = data[feature].astype(\n",
    "                        'float64', errors='ignore')\n",
    "                    invalid_value = self.get_invalid_float_value(data[feature])\n",
    "                    if invalid_value != '':\n",
    "                        invalid_values.add(invalid_value)\n",
    "                        self.features_with_invalid_value.append(feature)\n",
    "                    self.features_with_wrong_data_type.append(feature)\n",
    "                else:\n",
    "                    self.categorical_features.append(feature)\n",
    "\n",
    "            if data[feature].dtype == 'int64':\n",
    "                invalid_value = self.get_invalid_int_value(data[feature])\n",
    "                if invalid_value != '':\n",
    "                    invalid_values.add(invalid_value)\n",
    "                    self.features_with_invalid_value.append(feature)\n",
    "                data[feature] = data[feature].astype('float64', errors='raise')\n",
    "                self.numerical_features.append(feature)\n",
    "\n",
    "            elif data[feature].dtype == 'float64':\n",
    "                invalid_value = self.get_invalid_float_value(data[feature])\n",
    "                if invalid_value != '':\n",
    "                    invalid_values.add(invalid_value)\n",
    "                    self.features_with_invalid_value.append(feature)\n",
    "                self.numerical_features.append(feature)\n",
    "        return list(invalid_values)\n",
    "\n",
    "    def fit(self, data, y=None):\n",
    "\n",
    "        # Detect any numerical features with 'object' data type and with invalid values\n",
    "        print(f'Invalid values found: {self.find_invalid_values(data)}')\n",
    "\n",
    "        return self\n",
    "\n",
    "    def transform(self, data, y=None):\n",
    "\n",
    "        # Replace '?' value with NaN\n",
    "        data = data.replace('^\\?$', np.NaN, regex=True)\n",
    "\n",
    "        # Change numerical features with 'object' data type and change to 'float64'\n",
    "        for feature in self.features_with_invalid_value:\n",
    "            data[feature] = data[feature].astype('float64', errors='raise')\n",
    "\n",
    "        # Add the fixed features back to numerical features\n",
    "        self.numerical_features += self.features_with_invalid_value\n",
    "\n",
    "        # Remove unwanted quotes: change values like ''Example'' to 'Example'\n",
    "        for feature in self.categorical_features:\n",
    "            for value in data[feature].value_counts().index:\n",
    "                if re.search('^\\'.+\\'$', value.replace(' ', '')):\n",
    "                    data[feature].replace(value, value[1:-1], inplace=True)\n",
    "\n",
    "        # Drop any rows with null values for categorical attribute\n",
    "        for feature in self.categorical_features:\n",
    "            data = self.drop_rows_with_unknow_values(data, feature)\n",
    "\n",
    "        # Generate one-hot-encoded feature's names\n",
    "        for feature in self.categorical_features:\n",
    "            for value in data[feature].value_counts().index:\n",
    "                self.one_hot_features.append(f'{feature}_{value}')\n",
    "\n",
    "        self.numerical_features.remove(self.y_feature)\n",
    "\n",
    "        # Add mean value to the missing values for numerical attribute\n",
    "        numerical_pipeline = make_pipeline(\n",
    "            SimpleImputer(strategy='median'),\n",
    "            RobustScaler(),\n",
    "            MinMaxScaler()\n",
    "        )\n",
    "\n",
    "        categorical_pipleline = make_pipeline(\n",
    "            FunctionTransformer(lambda data: pd.get_dummies(data, columns=self.categorical_features,\n",
    "                                                            prefix=self.categorical_features))\n",
    "        )\n",
    "\n",
    "        data_preprocess_pipeline = make_column_transformer(\n",
    "            (numerical_pipeline, self.numerical_features),\n",
    "            (categorical_pipleline, self.categorical_features), remainder='passthrough')\n",
    "\n",
    "        data = pd.DataFrame(data_preprocess_pipeline.fit_transform(data),\n",
    "                            columns=self.numerical_features+self.one_hot_features+[self.y_feature])\n",
    "\n",
    "        return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "DYyuvkzElwR6"
   },
   "outputs": [],
   "source": [
    "def split_data(X, y, n_splits=1, test_size=0.2, random_state=42):\n",
    "\n",
    "    # split using stratified sampling\n",
    "    split = StratifiedShuffleSplit(\n",
    "        n_splits=n_splits, test_size=test_size, random_state=random_state)\n",
    "    train_index, test_index = next(split.split(X, y))\n",
    "\n",
    "    X_train, X_test, y_train, y_test = X.iloc[train_index], X.iloc[test_index], y[train_index], y[test_index]\n",
    "\n",
    "    return X_train, X_test, y_train, y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 87
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 12989,
     "status": "ok",
     "timestamp": 1599298630931,
     "user": {
      "displayName": "PolarBear Yap",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Gh6xY6wDRASo9ifgNeDcwMeBd3TCQFVGw14HLGtCg=s64",
      "userId": "07391035732959922581"
     },
     "user_tz": -480
    },
    "id": "VBvrjgfoAYLs",
    "outputId": "046c0b86-9bad-4917-a3ab-d52680ff7dbe"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Invalid values found: ['?']\n"
     ]
    }
   ],
   "source": [
    "Y_FEATURE = 'match'\n",
    "data_preprocess_pipeline = DataPreprocessTransformer(Y_FEATURE)\n",
    "dating_prepared = data_preprocess_pipeline.fit_transform(new_dating.copy())\n",
    "X_train, X_test, y_train, y_test = split_data(dating_prepared.drop(Y_FEATURE, axis=1),\n",
    "                                              dating_prepared[Y_FEATURE], test_size=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "RANDOM_SEED = 42"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "RANDOM_SEED = 42\n",
    "\n",
    "short_names = ['log_reg', 'decision_tree', 'rand_forest', 'extra_tree',\n",
    "               'ada_boost_cf', 'k_neighbors', 'c_naive_bayes',\n",
    "               'quadratic_dis_analysis', 'support_vector']\n",
    "\n",
    "names = ['Logistic Regression', 'Decision tree Classifier', 'Random forest Classifier',\n",
    "         'Extra Tree Classifier', 'AdaBoost Classifier', 'K Neighbors Classifier',\n",
    "         'Complement Naive Bayes', 'Quadratic Discriminant Analysis', 'C-Support Vector Classifier']\n",
    "\n",
    "functions = [\n",
    "    LogisticRegression(\n",
    "        penalty='none', random_state=RANDOM_SEED, max_iter=1000),\n",
    "    DecisionTreeClassifier(random_state=RANDOM_SEED),\n",
    "    RandomForestClassifier(random_state=RANDOM_SEED),\n",
    "    ExtraTreesClassifier(random_state=RANDOM_SEED),\n",
    "    AdaBoostClassifier(random_state=RANDOM_SEED),\n",
    "    KNeighborsClassifier(),\n",
    "    ComplementNB(),\n",
    "    QuadraticDiscriminantAnalysis(),\n",
    "    SVC(random_state=RANDOM_SEED, probability=True)\n",
    "]\n",
    "\n",
    "classifiers_idx = {}\n",
    "classifiers = {}\n",
    "\n",
    "# Zip all classfiers together into a dictionary for easy access\n",
    "for idx, s_name, name, func in zip(range(len(names)), short_names, names, functions):\n",
    "    classifiers_idx[idx] = {'name': name, 'func': func}\n",
    "    classifiers[s_name] = {'name': name, 'func': func}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_models_performace(classifiers, X, y):\n",
    "    train_results = {'classifier_name': [], 'duration': [],\n",
    "                     'precision': [], 'recall': [], 'f1_score': [], 'auc_score': []}\n",
    "    test_results = {'classifier_name': [], 'duration': [],\n",
    "                    'precision': [], 'recall': [], 'f1_score': [], 'auc_score': []}\n",
    "    for idx in range(len(classifiers)):\n",
    "        cf_name = classifiers[idx]['name']\n",
    "        print(f'{cf_name} has started...')\n",
    "        start = time.time()\n",
    "        cv_scores = cross_validate(classifiers[idx]['func'], X, y,\n",
    "                                   scoring=['f1', 'roc_auc', 'precision', 'recall'], cv=5,\n",
    "                                   return_train_score=True, n_jobs=-1)\n",
    "        end = time.time()\n",
    "        duration = end - start\n",
    "        print(f'{cf_name} ended in {duration} seconds.\\n')\n",
    "        updateRecord(train_results, cv_scores, 'train', cf_name, duration)\n",
    "        updateRecord(test_results, cv_scores, 'test', cf_name, duration)\n",
    "    return pd.DataFrame(train_results), pd.DataFrame(test_results)\n",
    "\n",
    "\n",
    "def updateRecord(df, scores, key_name, classifier_name, duration):\n",
    "    df['classifier_name'].append(classifier_name)\n",
    "    df['duration'].append(duration)\n",
    "    df['f1_score'].append(np.mean(scores[f'{key_name}_f1']))\n",
    "    df['auc_score'].append(np.mean(scores[f'{key_name}_roc_auc']))\n",
    "    df['precision'].append(np.mean(scores[f'{key_name}_precision']))\n",
    "    df['recall'].append(np.mean(scores[f'{key_name}_recall']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic Regression has started...\n",
      "Logistic Regression ended in 4.533708572387695 seconds.\n",
      "\n",
      "Decision tree Classifier has started...\n",
      "Decision tree Classifier ended in 2.7222795486450195 seconds.\n",
      "\n",
      "Random forest Classifier has started...\n",
      "Random forest Classifier ended in 2.8382511138916016 seconds.\n",
      "\n",
      "Extra Tree Classifier has started...\n",
      "Extra Tree Classifier ended in 2.446058988571167 seconds.\n",
      "\n",
      "AdaBoost Classifier has started...\n",
      "AdaBoost Classifier ended in 1.755713701248169 seconds.\n",
      "\n",
      "K Neighbors Classifier has started...\n",
      "K Neighbors Classifier ended in 7.593762397766113 seconds.\n",
      "\n",
      "Complement Naive Bayes has started...\n",
      "Complement Naive Bayes ended in 0.39484477043151855 seconds.\n",
      "\n",
      "Quadratic Discriminant Analysis has started...\n",
      "Quadratic Discriminant Analysis ended in 0.398914098739624 seconds.\n",
      "\n",
      "C-Support Vector Classifier has started...\n",
      "C-Support Vector Classifier ended in 14.477275609970093 seconds.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "train_results, test_results = get_models_performace(\n",
    "    classifiers_idx, X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../../pickles/model_performance.pickle', 'wb') as file:\n",
    "    pickle.dump(train_results, file)\n",
    "    pickle.dump(test_results, file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Grid Search"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "LD6LCuKCdu3t"
   },
   "source": [
    "## Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 105
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 128759,
     "status": "ok",
     "timestamp": 1599298747327,
     "user": {
      "displayName": "PolarBear Yap",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Gh6xY6wDRASo9ifgNeDcwMeBd3TCQFVGw14HLGtCg=s64",
      "userId": "07391035732959922581"
     },
     "user_tz": -480
    },
    "id": "gurZh7h1f_b1",
    "outputId": "93597ef3-2ea4-44f4-d871-3f2480cfec01"
   },
   "outputs": [],
   "source": [
    "# Logistics Regression with Default Settings\n",
    "log_reg = LogisticRegression(penalty='elasticnet', random_state=RANDOM_SEED,\n",
    "                             solver='saga', max_iter=3000, l1_ratio=0.5, C=1.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "km_gkJjp0ltC"
   },
   "outputs": [],
   "source": [
    "log_reg_param_grid = {\n",
    "    'l1_ratio': np.linspace(0, 1, 5),\n",
    "    'C': [0.01, 0.03, 0.1, 0.3, 1, 3, 10, 30]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 301
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 263238,
     "status": "ok",
     "timestamp": 1599298881877,
     "user": {
      "displayName": "PolarBear Yap",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Gh6xY6wDRASo9ifgNeDcwMeBd3TCQFVGw14HLGtCg=s64",
      "userId": "07391035732959922581"
     },
     "user_tz": -480
    },
    "id": "Bgfyx03C96UI",
    "outputId": "bc98ab4e-4f0e-44d2-bbac-059060614a78"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 40 candidates, totalling 120 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  34 tasks      | elapsed:    1.2s\n",
      "[Parallel(n_jobs=-1)]: Done 120 out of 120 | elapsed:  1.4min finished\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'C': 3, 'l1_ratio': 1.0}"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "log_grid_search = GridSearchCV(log_reg, param_grid=log_reg_param_grid,\n",
    "                               scoring='roc_auc', return_train_score=True, verbose=1, cv=3, n_jobs=-1)\n",
    "log_grid_search.fit(X_train, y_train)\n",
    "log_grid_search.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../../pickles/log_grid_search.pickle', 'wb') as file:\n",
    "    pickle.dump(log_grid_search, file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "4N9HcSyFdxYi"
   },
   "source": [
    "## Support Vector Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 87
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 333135,
     "status": "ok",
     "timestamp": 1599298952396,
     "user": {
      "displayName": "PolarBear Yap",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Gh6xY6wDRASo9ifgNeDcwMeBd3TCQFVGw14HLGtCg=s64",
      "userId": "07391035732959922581"
     },
     "user_tz": -480
    },
    "id": "tDtucNxdCpvh",
    "outputId": "5677c92b-9077-432f-8439-0cf5b2db2ed6"
   },
   "outputs": [],
   "source": [
    "# C-Support Vector Classifier with Default Settings\n",
    "svm_model = SVC(random_state=RANDOM_SEED, probability=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "i9F6BoPnCpvp"
   },
   "outputs": [],
   "source": [
    "# List of parameters and different combinations to produce a best result\n",
    "svm_param_grid = {'C': [0.1, 1, 10, 100, 1000, 10000],\n",
    "                  'gamma': [1, 0.1, 0.01, 0.001, 0.0001]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 779717,
     "status": "ok",
     "timestamp": 1599299399055,
     "user": {
      "displayName": "PolarBear Yap",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Gh6xY6wDRASo9ifgNeDcwMeBd3TCQFVGw14HLGtCg=s64",
      "userId": "07391035732959922581"
     },
     "user_tz": -480
    },
    "id": "jc6qTPOel5x9",
    "outputId": "276e35bd-ed2e-4ba6-a34e-b5444d2df46d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 30 candidates, totalling 90 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done   2 tasks      | elapsed:   14.8s\n",
      "[Parallel(n_jobs=-1)]: Done   9 tasks      | elapsed:   27.9s\n",
      "[Parallel(n_jobs=-1)]: Done  16 tasks      | elapsed:   29.8s\n",
      "[Parallel(n_jobs=-1)]: Done  25 tasks      | elapsed:   52.6s\n",
      "[Parallel(n_jobs=-1)]: Done  34 tasks      | elapsed:  1.1min\n",
      "[Parallel(n_jobs=-1)]: Done  45 tasks      | elapsed:  1.3min\n",
      "[Parallel(n_jobs=-1)]: Done  56 tasks      | elapsed:  1.5min\n",
      "[Parallel(n_jobs=-1)]: Done  69 tasks      | elapsed:  1.9min\n",
      "[Parallel(n_jobs=-1)]: Done  85 out of  90 | elapsed:  2.5min remaining:    8.8s\n",
      "[Parallel(n_jobs=-1)]: Done  90 out of  90 | elapsed:  2.6min finished\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'C': 100, 'gamma': 0.1}"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "svm_grid_search = GridSearchCV(svm_model, svm_param_grid, cv=3,\n",
    "                               scoring='roc_auc', return_train_score=True, verbose=10, n_jobs=-1)\n",
    "svm_grid_search.fit(X_train, y_train)\n",
    "svm_grid_search.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../../pickles/svm_grid_search.pickle', 'wb') as file:\n",
    "    pickle.dump(svm_grid_search, file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Validation Curve"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  24 out of  24 | elapsed:   30.7s finished\n"
     ]
    }
   ],
   "source": [
    "log_reg = LogisticRegression(penalty='elasticnet', random_state=RANDOM_SEED,\n",
    "                             solver='saga', l1_ratio=1.0, max_iter=3000)\n",
    "C_param = log_reg_param_grid['C']\n",
    "\n",
    "train_score, test_score = validation_curve(log_reg, X_train, y_train, param_name='C',\n",
    "                                           param_range=C_param, cv=3, scoring='roc_auc',\n",
    "                                           verbose=1, n_jobs=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../../pickles/log_val_C.pickle', 'wb') as file:\n",
    "    pickle.dump(train_score, file)\n",
    "    pickle.dump(test_score, file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Support Vector Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  18 out of  18 | elapsed:   45.5s finished\n"
     ]
    }
   ],
   "source": [
    "svm_model = SVC(random_state=42, probability=True, gamma=0.1)\n",
    "C_param = svm_param_grid['C']\n",
    "\n",
    "train_score, test_score = validation_curve(svm_model, X_train, y_train, param_name='C',\n",
    "                                           param_range=C_param, cv=3, scoring='roc_auc',\n",
    "                                           verbose=1, n_jobs=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../../pickles/tree_val_C.pickle', 'wb') as file:\n",
    "    pickle.dump(train_score, file)\n",
    "    pickle.dump(test_score, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  15 out of  15 | elapsed:   22.9s finished\n"
     ]
    }
   ],
   "source": [
    "svm_model = SVC(random_state=42, probability=True, C=100)\n",
    "gamma_param = svm_param_grid['gamma']\n",
    "\n",
    "train_score, test_score = validation_curve(svm_model, X_train, y_train, param_name='gamma',\n",
    "                                           param_range=gamma_param, cv=3, scoring='roc_auc',\n",
    "                                           verbose=1, n_jobs=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../../pickles/tree_val_gamma.pickle', 'wb') as file:\n",
    "    pickle.dump(train_score, file)\n",
    "    pickle.dump(test_score, file)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Navigation",
   "toc_cell": true,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
